<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Variational Inference for Latent Variable Models | Yi-Shiuan Tung</title> <meta name="author" content="Yi-Shiuan Tung"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?e26219d56e20b1927be1ba2824b250be"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yi-shiuan-tung.github.io/blog/2025/vae/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yi-Shiuan </span>Tung</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Variational Inference for Latent Variable Models</h1> <p class="post-meta">September 15, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> </p> </header> <article class="post-content"> <div id="markdown-content" style="text-align: justify;"> <p>This post goes through the derivation of Evidence Lower Bound (ELBO) and an intuitive explanation of how variational inference works for latent variable models. Much of the intuition presented here is inspired by <a href="https://youtu.be/UTMpM4orS30?si=BzBurdtiXm5orxEo" rel="external nofollow noopener" target="_blank">Sergey Levine’s lectures on variational inference</a> and insights from discussions with <a href="https://www.colorado.edu/cs/christoffer-heckman" rel="external nofollow noopener" target="_blank">Chris Heckman</a> during my area exam.</p> <h3 id="latent-variable-models">Latent Variable Models</h3> <p>Suppose we want to model a complex data distribution \(p(x)\) given a dataset \(D = \{x_1, x_2, \ldots, x_N\}\) where \(x\) might represent images, robot trajectories, or other high-dimensional data. Directly modeling \(p(x)\) is often intractable due to its complexity.</p> <p>Latent variable models address this challenge by introducing an auxiliary random variable \(z\) drawn from a simple distribution \(p(z)\), such as a Gaussian. Rather than modeling \(p(x)\) directly, we instead model how data is generated <em>conditioned</em> on the latent variable via \(p(x\vert z)\).</p> <p>The conditional distribution \(p(x \vert z)\) is chosen to be easy to sample from. A common choice is also Gaussian:</p> <p>\(\begin{equation} p(x \vert z) = \mathcal{N}(\mu(z), \sigma(z)) \end{equation}\),</p> <p>where the mean \(\mu(z)\) and standard deviation \(\sigma(z)\) are functions of \(z\) learned from data. The marginal distribution over observations is then obtained by integrating out the latent variable:</p> <p>\(\begin{equation} p(x) = \int p(x \vert z)p(z)dz \end{equation}\).</p> <center> <img src="/blog/assets/img/vae/intro.png" alt="Latent variable model illustration" width="280"> </center> <p>Sampling a latent variable \(z\) selects a Gaussian distribution over the data space via \(p(x \vert z)\). By drawing different values of \(z\) and sampling from the corresponding conditionals, the model can represent complex data distributions.</p> <h3 id="how-do-we-train-the-model-p_thetax">How do we train the model \(p_{\theta}(x)\)?</h3> <p>We can use maximum likelihood to train the model \(p_{\theta}(x)\) where \(\theta\) are the parameters of the model. However, the integration over \(z\) is intractable.</p> <p>\(\begin{align} \mathcal{L}(\theta) &amp;= \sum_{i=1}^{N} \log p_{\theta}(x_i) \\ &amp;= \sum_{i=1}^N \log \int p(x_i\vert z)p(z)dz\\ \end{align}\).</p> <p><strong>Why is the integration over \(z\) intractable?</strong> The integration over \(z\) is intractable because \(p(x \vert z)\) depends nonlinearly on \(z\). In deep latent variable models, \(p(x \vert z)\) is typically parameterized as a Gaussian whose mean and variance are outputs of a neural netowrk.</p> \[\begin{equation} p(x \vert z) = \mathcal{N}(\mu_{nn}(z), \sigma_{nn}(z)) \end{equation}\] <p>Because neural networks introduce nonlinear dependencies on \(z\), the integrand is no longer a Gaussian in \(z\), and the resulting integral has no closed-form solution.</p> <p>Closed-form marginalization is only possible in restricted settings. For example, if \(p(z) = \mathcal{N}(0, I)\) and \(p(x \vert z) = \mathcal{N}(Az+b, \Sigma)\), then the model is linear-Gaussian, and the marginal distribution is \(p(x) = \mathcal{N}(b, AA^T + \Sigma)\).</p> <p><strong>What if \(z\) is discrete?</strong> If \(z\) were discrete, the integral would become a sum, but computing gradients of the log-likelihood would still require evaluating or summing over all latent states, which quickly becomes infeasible in large or structured latent spaces.</p> <p><strong>Why can’t we sample \(z\) to approximate the integral and gradients?</strong> We could approximate the marginal likelihood using Monte Carlo sampling: \(\begin{equation} p(x) \approx \frac{1}{M}\sum_{i=1}^M p(x \vert z_i), \quad z_i \sim p(z) \end{equation}\)</p> <p>However, maximum likelihood requires gradients of \(\text{log}p(x)\) which depend on the posterior \(p(z \vert x)\). Sampling from the prior \(p(z)\) does not provide samples from the posterior. As a result, naive Monte Carlo sampling results in high variance estimates, leading to unstable and impractical learning. Below is the derivation for why \(\nabla_{\theta} \text{log}p_{\theta}(x)\) depends on \(p(z \vert x)\).</p> \[\nabla_{\theta} \text{log}p_{\theta}(x) = \frac{1}{p_{\theta}(x)} \nabla_{\theta} p_{\theta}(x) = \frac{1}{p_{\theta}(x)} \int \nabla_{\theta} p_{\theta}(x \vert z)p(z)dz\] <p>Now, if we take the derivative of \(\text{log}p_{\theta}(x \vert z)\) with respect to \(\theta\), we get \(\nabla_{\theta} \text{log}p_{\theta}(x \vert z) = \frac{1}{p_{\theta}(x \vert z)} \nabla_{\theta} p_{\theta}(x \vert z)\). Then we get \(\nabla_{\theta}p_{\theta}(x \vert z) = p_{\theta}(x \vert z) \nabla_{\theta} \text{log}p_{\theta}(x \vert z)\). Substituting this into the gradient of \(\text{log}p_{\theta}(x)\), we get</p> \[\begin{equation} \nabla_{\theta} \text{log}p_{\theta}(x) = \frac{1}{p_{\theta}(x)} \int p_{\theta}(x \vert z) p(z) \nabla_{\theta} \text{log}p_{\theta}(x \vert z)dz \end{equation}\] <p>We have the term \(\frac{p_{\theta}(x \vert z)p(z)}{p_{\theta}(x)}\), which is exactly the posterior \(p_{\theta}(z \vert x)\). The gradient becomes</p> \[\begin{align} \nabla_{\theta} \text{log}p_{\theta}(x) &amp;= \int p_{\theta}(z \vert x) \nabla_{\theta} \text{log}p_{\theta}(x \vert z)p(z)dz\\ &amp;= \mathbb{E}_{p_{\theta}(z \vert x)}\left[\nabla_{\theta} \text{log}p_{\theta}(x \vert z)\right] \end{align}\] <h3 id="variational-approximation">Variational Approximation</h3> <p>This motivates the use of variational inference, which replaces the intractable posterior \(p(z \vert x)\) with a tractable approximation \(q_{\phi}(z \vert x)\) and yields a differentiable lower bound on the log-likelihood. Here we go through the derivation of the ELBO (Evidence Lower Bound). Starting with the log-likelihood which we want to maximize with respect to \(\theta\), we have</p> \[\begin{equation} \text{log}p_{\theta}(x) = \text{log} \int p_{\theta}(x \vert z)p(z)dz \end{equation}\] <p>We introduce a variational distribution \(q_{\phi}(z \vert x)\), which is parameterized by \(\phi\) and approximates the posterior \(p_{\theta}(z \vert x)\). The key idea is to rewrite the marginal likelihood in a way that allows us to take expectations with respect to \(q_{\phi}(z \vert x)\), which we can sample from. We multiply the above equation by \(\frac{q_{\phi}(z \vert x)}{q_{\phi}(z \vert x)}\) and use the fact that \(\int q_{\phi}(z \vert x)dz = 1\) to get</p> \[\begin{align} \text{log}p_{\theta}(x) &amp;= \text{log} \int p_{\theta}(x \vert z)p(z)dz\\ &amp;= \text{log} \int p_{\theta}(x \vert z)q_{\phi}(z \vert x)\frac{p(z)}{q_{\phi}(z \vert x)}dz\\ &amp;= \text{log} E_{z \sim q_{\phi}(z \vert x)}\left[\frac{p_{\theta}(x \vert z)p(z)}{q_{\phi}(z \vert x)}\right] \end{align}\] <p>The logarithm is a concave function, so we can apply Jensen’s inequality to get</p> \[\begin{align} \text{log}p_{\theta}(x) &amp;= \text{log} E_{z \sim q_{\phi}(z \vert x)}\left[\frac{p_{\theta}(x \vert z)p(z)}{q_{\phi}(z \vert x)}\right]\\ &amp;\geq E_{z \sim q_{\phi}(z \vert x)}\left[\text{log}\frac{p_{\theta}(x \vert z)p(z)}{q_{\phi}(z \vert x)}\right]\\ &amp;= \mathbb{E}_{z \sim q_{\phi}(z \vert x)}\left[\text{log}p_{\theta}(x \vert z) + \text{log}p(z) - \text{log}q_{\phi}(z \vert x)\right]\\ \end{align}\] <p>The right hand side is the ELBO, which is a lower bound on the log-likelihood. It is a differentiable function of \(\theta\) and \(\phi\), and can be used to train the model.</p> <p><strong>What makes a good \(q_{\phi}(z \vert x)\)?</strong></p> <p>The intuition is that \(q_{\phi}(z \vert x)\) should be close to \(p_{\theta}(z \vert x)\), and we can use KL-divergence to measure the difference between the two distributions.</p> \[\begin{align} D_{KL}(q_{\phi}(z \vert x) \vert \vert p_{\theta}(z \vert x)) &amp;= \mathbb{E}_{z \sim q_{\phi}(z \vert x)}\left[\text{log}\frac{q_{\phi}(z \vert x)}{p_{\theta}(z \vert x)}\right]\\ &amp;= \mathbb{E}_{z \sim q_{\phi}(z \vert x)}\left[\text{log}q_{\phi}(z \vert x) - \text{log}p_{\theta}(z \vert x)\right] \end{align}\] <p>So we can rewrite the ELBO as</p> \[\begin{align} \text{ELBO}(\theta, \phi) &amp;= \mathbb{E}_{z \sim q_{\phi}(z \vert x)}\left[\text{log}p_{\theta}(x \vert z) + \text{log}p(z) - \text{log}q_{\phi}(z \vert x)\right]\\ &amp;= \mathbb{E}_{z \sim q_{\phi}(z \vert x)}\left[\text{log}p_{\theta}(x \vert z) - D_{KL}(q_{\phi}(z \vert x) \vert \vert p_{\theta}(z \vert x)) \right] \end{align}\] <p>To maximize \(p_{\theta}(x)\), we can maximize the ELBO with respect to \(\theta\) and \(\phi\). Intuitively, ELBO balances two objectives: the reconstruction loss \(\text{log}p_{\theta}(x \vert z)\) and the KL-divergence \(D_{KL}(q_{\phi}(z \vert x) \vert \vert p_{\theta}(z \vert x))\). The reconstruction loss encourages \(q_{\phi}(z \vert x)\) to place mass on latent variables that reconstruct \(x\) well, while the KL-divergence encourages \(q_{\phi}(z \vert x)\) to be close to \(p(z)\).</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/min-entropy/">Minimizing Entropy for Classification Problems</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/workspace-optimization/">Workspace Optimization Techniques to Improve Human Motion Prediction</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Yi-Shiuan Tung. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: December 19, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-509JKBBD7E"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-509JKBBD7E");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>