<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Counterfactual Reasoning and Environment Design for Active Preference Learning | Yi-Shiuan Tung</title> <meta name="author" content="Yi-Shiuan Tung"> <meta name="description" content="Blog post for RSS'25 Human-in-the-Loop Robot Learning Workshop"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?e26219d56e20b1927be1ba2824b250be"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yi-shiuan-tung.github.io/blog/2025/cred/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yi-Shiuan </span>Tung</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Counterfactual Reasoning and Environment Design for Active Preference Learning</h1> <p class="post-meta">June 26, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/publications"> <i class="fa-solid fa-hashtag fa-sm"></i> publications</a>   </p> </header> <article class="post-content"> <div id="markdown-content" style="text-align: justify;"> <p><strong>Yi-Shiuan Tung</strong>, <strong>Bradley Hayes</strong>, and <strong>Alessandro Roncone</strong><br> University of Colorado Boulder<br> <a href="https://hitl-robot-learning.github.io/pdfs/cred.pdf" rel="external nofollow noopener" target="_blank">Workshop Paper PDF</a></p> <hr> <h2 id="overview">Overview</h2> <center> <img src="/blog/assets/img/pref_learning/husky_question.png" alt="Delivery robot has multiple options for routes to take" width="310"> </center> <p><br></p> <p>Robots deployed in the real world must align their behaviors with human preferences—whether balancing speed and safety in delivery tasks or adapting routes based on distance, time, and terrain. But those preferences are hard to predefine and differ across users.</p> <p><strong>Active Preference Learning (APL)</strong> helps robots learn these preferences by asking users to compare and rank trajectories. To improve sample efficiency, we present the human with trajectory pairs that maximize information gain [1]. The objective minimizes the difference between the entropy of the reward distribution (\(H(w)\)) before and after getting human input (\(I\)).</p> <p>\(\begin{equation} \max_{\xi_A, \xi_B} f(\xi_A, \xi_B) = \max_{\xi_A, \xi_B} H(\mathbf{w}) - \mathbb{E}_{I}[H(\mathbf{w} | I)] \end{equation}\).</p> <p>To make the optimization tractable, prior work use a pre-generated set of trajectories from random rollouts [1] or rollouts from a replay buffer [2] to find trajectory pair that optimizes information gain. However, this is <strong>sample inefficient</strong> for long-horizon tasks because the number of possible trajectories grow exponentially as the number of horizon increases. For robot routing, we also have to query the human for preferences in different environments or scenarios to enable <strong>generalization</strong>. Therefore, we include the environment parameters as optimization variables.</p> <p>We propose <strong>CRED</strong>, a method that improves preference learning by:</p> <ul> <li>Using <strong>Counterfactual Reasoning</strong> to generate queries with trajectories that represent different preferences.</li> <li>Performing <strong>Environment Design</strong> to create “imagined” environments that better elicit informative preferences.</li> </ul> <p>CRED significantly improves sample efficiency and generalization across both simulated GridWorld and OpenStreetMap navigation.</p> <hr> <h2 id="method">Method</h2> <h3 id="1-counterfactual-reasoning">1. Counterfactual Reasoning</h3> <p>When asking humans for preferences, we hypothesize that the trajectories should represent different preferences. To do that, CRED samples potential human reward functions from the current Bayesian belief over weights (w), and generates trajectories that would be optimal if those weights were true. It then evaluates pairs of these counterfactual trajectories to find the most informative preference queries—those that maximize <a href="#overview">Eq. 1</a>.</p> <center> <img src="/blog/assets/img/pref_learning/cr.png" alt="Counterfactual reasoning samples rewards from current belief to generate trajectories that resemble different human preferences." width="600"> </center> <p><br></p> <h3 id="2-environment-design">2. Environment Design</h3> <p>The environment affects which preferences can be expressed. For example, distinguishing between preferences for “paved vs. gravel” requires an environment with both terrains.</p> <p>CRED uses <strong>Bayesian Optimization</strong> to find environment configurations that maximize the informativeness of queries. In practice, this means modifying terrain layouts or edge attributes (e.g., road slope or elevation) to elicit more useful feedback. Bayesian optimization uses a Gaussian process to guide its search, reducing the number of evaluations of <a href="#overview">Eq. 1</a>. Here, \(F\) is Eq. 1 but includes environment parameters \(\theta_E\) as optimization variables.</p> <center> <img src="/blog/assets/img/pref_learning/env_design.png" alt="Bayesian optimization finds an environment to query the human." width="800"> </center> <p><br></p> <hr> <h2 id="experiments">Experiments</h2> <p>We evaluate CRED in two domains:</p> <h3 id="gridworld-navigation">GridWorld Navigation</h3> <p>A 15×15 terrain-based environment with brick (red), gravel (grey), sand (yellow), grass (green), and paved (white). The goal is to move from the top left corner to the bottom right corner. For environment design, we first compress the 15x15 grid to a 5 dimensional vector using variational autoencoder.</p> <center> <img src="/blog/assets/img/pref_learning/gridworld_1.png" alt="GridWorld 1" width="150"> <img src="/blog/assets/img/pref_learning/gridworld_2.png" alt="GridWorld 2" width="150"> <img src="/blog/assets/img/pref_learning/gridworld_3.png" alt="GridWorld 3" width="150"> <img src="/blog/assets/img/pref_learning/gridworld_4.png" alt="GridWorld 4" width="150"> <img src="/blog/assets/img/pref_learning/gridworld_5.png" alt="GridWorld 5" width="150"> </center> <h3 id="openstreetmap-routing">OpenStreetMap Routing</h3> <p>We use OpenStreetMap to extract nodes representing intersections and edges representing streets. We evaluate the algorithm’s ability to learn preferences for distance, time, and elevation (+/-). For environment design, we modify edge attributes such as traversal time (i.e. traffic) and elevation and evaluate generalization to new street networks.</p> <center> <img src="/blog/assets/img/pref_learning/StreetNav_boulder.png" alt="Boulder" width="150"> <img src="/blog/assets/img/pref_learning/StreetNav_east_boulder.png" alt="East Boulder" width="150"> <img src="/blog/assets/img/pref_learning/StreetNav_south_boulder.png" alt="South Boulder" width="150"> </center> <hr> <h2 id="baselines">Baselines</h2> <p>We compare CRED to:</p> <ul> <li> <strong>RR (Random Rollouts):</strong> Pre-generated set of trajectories through random rollout [1].</li> <li> <strong>MBP (Mean Belief Policy):</strong> Uses the mean of the belief over rewards as the best guess and perform rollouts with policy trained on the reward [3].</li> <li> <strong>CR (Counterfactual Reasoning only):</strong> Ablation of CRED without environment design.</li> <li> <strong>MBP + ED:</strong> Mean Belief Policy combined with environment design.</li> </ul> <hr> <h2 id="key-results">Key Results</h2> <h3 id="-qualitative-results">👀 Qualitative Results</h3> <center> <video width="250" controls=""> <source src="/blog/assets/img/pref_learning/GridWorld-v0_3_query_5.mp4" type="video/mp4"></source> Your browser does not support the video tag. <figcaption>Mean Belief Policy [3]</figcaption> </video> <video width="250" controls=""> <source src="/blog/assets/img/pref_learning/GridWorld-v0_3_query_4.mp4" type="video/mp4"></source> Your browser does not support the video tag. <figcaption>Counterfactual Reasoning</figcaption> </video> <video width="250" controls=""> <source src="/blog/assets/img/pref_learning/GridWorld-v0_3_query_0.mp4" type="video/mp4"></source> Your browser does not support the video tag. <figcaption>Our Approach: CRED</figcaption> </video> </center> <p><br></p> <p>While Mean Belief Policy [3] (left) can generate trajectories with different features, the trajectories are very similar. Counterfactual reasoning (middle) generates trajectories that better resemble different preferences. With environment design (right), we can query the human for feedback in different environments.</p> <h3 id="-higher-information-gain">🔍 Higher Information Gain</h3> <center> <img src="/blog/assets/img/pref_learning/GridWorld-v0_objective_values.png" alt="GridWorld Objective Values" width="230"> <img src="/blog/assets/img/pref_learning/GridWorld-v0_entropy.png" alt="GridWorld Entropy" width="230"> <img src="/blog/assets/img/pref_learning/SimpleStreetNav-v0_objective_values.png" alt="StreetNav Objective Values" width="230"> <img src="/blog/assets/img/pref_learning/SimpleStreetNav-v0_entropy.png" alt="StreetNav Entropy" width="230"> </center> <center> <img src="/blog/assets/img/pref_learning/legend.png" width="400"> </center> <p>Left to right: GridWorld information gain, entropy of belief over reward weights, OpenStreetMap information gain, and entropy of belief over reward weights across different iterations of querying the human for feedback. CRED generates more informative preference queries early on, resulting in lower entropy of the belief over rewards.</p> <h3 id="-higher-generalization">✅ Higher Generalization</h3> <p>CRED-trained policies perform better in unseen environments, demonstrating faster convergence and higher rewards and policy accuracy.</p> <center> <img src="/blog/assets/img/pref_learning/pref_results.png" alt="Results" width="1000"> </center> <hr> <h2 id="conclusion">Conclusion</h2> <p>We introduce CRED for active preference learning which improves the sample efficiency and generalization of the learned reward functions. Counterfactual reasoning generates queries with trajectories that better resemble different reward functions. By using environment design, we can jointly optimize the environment and query generation, enabling the ability to query the human in different environments.</p> <hr> <h2 id="acknowledgments">Acknowledgments</h2> <p>Thanks to Dusty Woods for help with visualizations and figure editing.</p> <hr> <h2 id="references">References</h2> <p>[1] Biyik, E., Palan, M., Landolfi, N. C., Losey, D. P., &amp; Sadigh, D. (2020). <em>Asking easy questions: A user-friendly approach to active reward learning</em>. CoRL.</p> <p>[2] Lee, K., Smith, L. M., &amp; Abbeel, P. (2021). <em>PEBBLE: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training</em>. ICML.</p> <p>[3] Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., &amp; Amodei, D. (2017). <em>Deep reinforcement learning from human preferences</em>. NeurIPS.</p> <hr> <h2 id="contact">Contact</h2> <p>Questions or collaboration ideas?<br> 📧 yi-shiuan.tung@colorado.edu</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/workspace-optimization/">Workspace Optimization Techniques to Improve Human Motion Prediction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/min-entropy/">Minimizing Entropy for Classification Problems</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/robotic-kitting/">Bilevel Optimization for Just-in-Time Robotic Kitting</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Yi-Shiuan Tung. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: October 31, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-509JKBBD7E"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-509JKBBD7E");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>